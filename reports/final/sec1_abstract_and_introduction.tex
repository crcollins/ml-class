\begin{abstract}
To be filled.    
\end{abstract}
\section{Introduction}
%\subsection{}
\noindent Quantum mechanics enables us to calculate chemical compounds' properties to a very high accuracy. Unfortunately, such calculations typically involve solving certain nonlinear ill-conditioned partial differential equations, which is extremely time-consuming. For example, to compute the total energy of a given molecule, there are Density Functional Theory (DFT) or \textit{ab initio} theory available for this task; however, they scale $O(n^4)$ to $O(n!)$ asymptotically in CPU time. For a medium-sized molecule that consists of ~100 atoms, one may need tens of hours of CPU time on a modern scientific computing cluster. Often in chemistry research, people deal with chemicals that consist of thousands of atoms (or hundreds of thousands in the case of biochemistry), which requires weeks/months of CPU time. There is thus a need for theories that have better asymptotic scaling rate yet retain an acceptable high level of accuracy. Traditionally, the way to encounter such a dilemma is to introduce approximations in quantum mechanics equations, which leads to Semi-Empirical Quantum Chemistry (SEQC) methods. SEQC methods scale as low as $O(n^2)$, but can only achieve qualitative accuracy. \\

Recent studies show that a variety of machine learning regression models can be employed to predict chemical properties. Instead of solving partial differential equations directly, machine learning models try to predict chemical properties from "chemical similarities". From a chemist's point of view, chemicals are made of only a small set of structural building blocks, the properties of which have been thoroughly studied. Molecules consisted of similar building blocks are very likely to share similar chemical properties. Therefore, it is reasonable to train our machine learning models with features come from chemicals made of only a few building blocks, then use these models to predict properties of chemicals that constitute a great number of building blocks. An analogy is to train a spam filter with e-mails that have only a few sentences, and hope that this filter can classify not only short e-mails but also long e-mails that have similar sentences as our training examples do. Unlike quantum mechanical models which are considered "universal", our models will be, for sure, only applicable to a subset of chemicals, but that is just the inevitable trade-off between accuracy and generality. \\

Philosophically, our feature engineering part is highly related with our chemistry knowledge about our target molecules. It is expected that we can extract chemically informative characteristics of every molecule in both training stage and test stage. We have implemented 6 different ways to transform structural features of all the sample molecules into feature vectors. On the other hand, our training labels are supposed to be generated through quantum mechanical calculations, which require a solid understanding of the physics behind. Right now we basically treat this type of calculation as a black box, but in quantum chemistry there exist ways about how to reformulate complicated partial differential equations into simple ones--that is what most of the SEQC methods do, and we are planning to take advantage of this point in feature engineering. Specifically, we are interested in establishing regression models of the energies of highest occupied molecular orbital (HOMO), lowest unoccupied molecular orbital (LUMO), and band gap. These 3 properties are of great chemistry/physics interests both theoretically and experimentally, since they are directly related with molecules' spectroscopic behaviors. Our goal is to predict these chemical properties with machine learning methods that are cheaper, and produce ``chemical accuracy" (1 kcal/mol (0.047 eV)).\\

In this report, several common polymer chains are constructed as samples and their HOMO, LUMO and band gap are calculated as labels. We develop 9 different types of feature vectors to employ supervised machine learning methods, including Linear Regression, Linear Ridge Regression, Support Vector Machines (SVM), K-Nearest Neighbors (k-NN), Decision Tree, Boosting and Neural Networks. Details on dataset construction, feature engineering, methodology and model developments are demonstrated in the following sections. We will also present corresponding training/testing errors of combinations of different feature vectors and machine learning models later in this report. \\
