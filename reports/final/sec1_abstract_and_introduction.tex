\begin{abstract}
TEMP

Density Functional Theory (DFT) has been used extensively for predicting chemical properties as a compromise between accurate (but computationally expensive) ab initio methods and cheap Semi-Emperical Quantum Chemistry (SEQC) methods. Unfortunately, these DFT methods do not scale as well as would be desired (often being restricted to molecules with hundreds of atoms). Unfortunately, often in chemistry research, the structures of interest are on the order of hundreds of thousands of atoms. With all of these models currently, they do not make use of similar structural features in molecules when calculating their properties. The goal of our work is to create a machine learning model that takes advantage of molecular similarity to produce highly accurate, but computationally cheap results. Specifically, we will be looking at three molecular properties: the highest occupied molecular orbital (HOMO), the lowest unoccupied molecular orbital (LUMO), and the band gap energies. These properties are of great interest because they are directly related to a molecules' spectroscopic behaviors and overall reactivity. Our goal is to predict these chemical properties with machine learning methods that are cheaper, and produce "chemical accuaracy" (1 kcal/mol (0.043 eV)).

TEMP
\end{abstract}
\section{Introduction}
%\subsection{}
\noindent Quantum mechanics enables us to calculate chemical compounds' properties to a very high accuracy. Unfortunately, such calculations typically involve solving certain nonlinear ill-conditioned partial differential equations, which is extremely time-consuming. For example, to compute the total energy of a given molecule, there are Density Functional Theory (DFT) or \textit{ab initio} theory available for this task; however, they scale $O(n^4)$ to $O(n!)$ asymptotically in CPU time. For a medium-sized molecule that consists of ~100 atoms, one may need tens of hours of CPU time on a modern scientific computing cluster. Often in chemistry research, people deal with chemicals that consist of thousands of atoms (or hundreds of thousands in the case of biochemistry), which requires weeks or months of CPU time. There is thus a need for theories that have better asymptotic scaling rate yet retain an acceptable high level of accuracy. Traditionally, the way to encounter such a dilemma is to introduce approximations in quantum mechanics equations, which leads to Semi-Empirical Quantum Chemistry (SEQC) methods. SEQC methods scale as low as $O(n^2)$, but can only achieve qualitative accuracy. \\

Recent studies show that a variety of machine learning regression models can be employed to predict chemical properties. Instead of solving partial differential equations directly, machine learning models try to predict chemical properties from "chemical similarities". From a chemist's point of view, chemicals are made of only a small set of structural building blocks, the properties of which have been thoroughly studied. Molecules consisted of similar building blocks are very likely to share similar chemical properties. Therefore, it is reasonable to train our machine learning models with features come from chemicals made of only a few building blocks, then use these models to predict properties of chemicals that constitute a great number of building blocks. An analogy is to train a spam filter with e-mails that have only a few sentences, and hope that this filter can classify not only short e-mails but also long e-mails that have similar sentences as our training examples do. Unlike quantum mechanical models which are considered "universal", our models will be, for sure, only applicable to a subset of chemicals, but that is just the inevitable trade-off between accuracy and generality. \\

Philosophically, our feature engineering part is highly related with our chemistry knowledge about our target molecules. It is expected that we are able to extract chemically informative characteristics from every molecule in both training stage and test stage. We have implemented 6 different ways to transform structural features of all the sample molecules into feature vectors. On the other hand, our training labels are supposed to be generated through DFT calculations. Specifically, we are interested in establishing regression models of the energies of highest occupied molecular orbital (HOMO), lowest unoccupied molecular orbital (LUMO), and band gap. These 3 properties are of great chemistry/physics interests both theoretically and experimentally, since they are directly related with molecules' spectroscopic behaviors. Our goal is to predict these chemical properties with efficient machine learning regression methods, and reduce our error down to "chemical accuracy", which is numerically equal to 1 kcal/mol or 0.043 eV, and is considered as the inevitable error if people experimentally measure these properties. Theoretical models that achieve this level of error is typically considered as valid replacements of chemistry experiments that are of great use. \\

In addition, as we mentioned before, in quantum chemistry there exist a lot of SEQC methods that scale significantly better than DFT/\textit{ab initio} but also have systematically larger error rates; yet it is very likely that some machine learning regression models can help us reduce that error if enough training data are given. By employing quantum mechanics perturbation theory, we are able to establish a linear relationship between the properties calculated from SEQC methods and the regression labels calculated from DFT. Detailed discussion about how we utilize SEQC methods in feature engineering can be found later in the feature engineering section. \\

Data set wise, several types common polymer chains are constructed as samples; their HOMO, LUMO and band gap (energies) are calculated as labels. We have developed 9 different types of feature vectors to employ supervised machine learning methods, including Linear Regression, Linear Ridge Regression, Support Vector Machines (SVM), K-Nearest Neighbors (k-NN), Decision Tree, Boosting and Neural Networks. Details on dataset construction, feature engineering, methodology and model developments are demonstrated in the following sections. We hereby present corresponding training/testing errors of combinations of different feature vectors and machine learning models later in the result section. \\
